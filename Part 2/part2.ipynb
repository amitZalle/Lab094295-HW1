{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing"
      ],
      "metadata": {
        "id": "PUMcJr3G92vb"
      },
      "id": "PUMcJr3G92vb"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "PeH8ZFcRajY2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeH8ZFcRajY2",
        "outputId": "6ef87dfd-9011-4a49-8da9-b3302b2de0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from Faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Faiss-cpu) (24.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install Faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import faiss\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "j8ax0sS4ULbF"
      },
      "id": "j8ax0sS4ULbF",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "62f09c59a02a3b0d",
      "metadata": {
        "id": "62f09c59a02a3b0d"
      },
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a991f1eb012a476",
      "metadata": {
        "id": "5a991f1eb012a476"
      },
      "outputs": [],
      "source": [
        "def semi_optimized_exhaustive_search(\n",
        "        index_vectors: np.ndarray,\n",
        "        query_vectors: np.ndarray,\n",
        "        k: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function performs an optimized exhaustive search.\n",
        "    Args:\n",
        "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "        query_vectors: An array of shape (n_queries, dim) containing the query vectors.\n",
        "        dim: The dimensionality of the vectors.\n",
        "    Returns:\n",
        "        An array of shape (n_queries, k) containing the indices of the k nearest neighbors for each query vector.\n",
        "    \"\"\"\n",
        "    ann_lists = []\n",
        "    for query_vec in query_vectors:\n",
        "        distances = np.linalg.norm(index_vectors - query_vec, axis=1)\n",
        "        ann_lists.append(list(np.argsort(distances)[:k]))\n",
        "    return np.array(ann_lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a8ef475c717fbe2e",
      "metadata": {
        "id": "a8ef475c717fbe2e"
      },
      "outputs": [],
      "source": [
        "def build_faiss_flatl2_index(\n",
        "        index_vectors: np.ndarray,\n",
        "        dim: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function builds a Faiss flat L2 index.\n",
        "    Args:\n",
        "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "        dim: The dimensionality of the vectors.\n",
        "    Returns:\n",
        "        A Faiss flat L2 index.\n",
        "    \"\"\"\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(index_vectors)\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1df7a2d698755a82",
      "metadata": {
        "id": "1df7a2d698755a82"
      },
      "outputs": [],
      "source": [
        "def faiss_search(\n",
        "        query_vectors: np.ndarray,\n",
        "        index: faiss.Index,\n",
        "        k: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function uses a Faiss index to search for the k-nearest neighbors of query_vectors.\n",
        "    Args:\n",
        "        query_vectors: An array of shape (n_queries, dim) containing the query vectors.\n",
        "        index: A Faiss index.\n",
        "        k: The number of nearest neighbors to retrieve.\n",
        "    Returns:\n",
        "        An array of shape (, ) containing the indices of the k-nearest neighbors for each query vector.\n",
        "    \"\"\"\n",
        "    distances, indices = index.search(query_vectors, k)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "af14bea64023a3d4",
      "metadata": {
        "id": "af14bea64023a3d4"
      },
      "outputs": [],
      "source": [
        "def build_faiss_lsh_index(\n",
        "        index_vectors: np.ndarray,\n",
        "        dim: int,\n",
        "        nbits: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function builds a Faiss LSH index.\n",
        "    Args:\n",
        "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "        dim: The dimensionality of the vectors.\n",
        "        nbits: The number of bits to use in the hash.\n",
        "    Returns:\n",
        "        A Faiss LSH index.\n",
        "    \"\"\"\n",
        "    index = faiss.IndexLSH(dim, nbits)\n",
        "    index.add(index_vectors)\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b4b0932dfa7d7a4c",
      "metadata": {
        "id": "b4b0932dfa7d7a4c"
      },
      "outputs": [],
      "source": [
        "def compute_recall_at_k(\n",
        "        nn_gt: np.ndarray,\n",
        "        ann: np.ndarray,\n",
        "        k: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function computes the recall@k.\n",
        "    Args:\n",
        "        nn_gt: The ground truth nearest neighbors.\n",
        "        ann: The approximate nearest neighbors.\n",
        "        k: The number of nearest neighbors to consider.\n",
        "    Returns:\n",
        "        The recall@k.\n",
        "    \"\"\"\n",
        "    return round(sum([len(set(ann[i]) & set(nn_gt[i])) / k for i in range(len(ann))])/len(ann), 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d4be2e90ed842",
      "metadata": {
        "id": "77d4be2e90ed842"
      },
      "source": [
        "# 2.1 -- LSH vs Naive Exhaustive Search (Regular Index Vectors)\n",
        "### You just have to run the following cells and add the following results to the report:\n",
        "* running time of the ground truth computation with semi_optimized_exhaustive_search (wall time)\n",
        "* running time of creating faiss_lsh_index (wall time)\n",
        "* running time of faiss_search over query_vectors with faiss_lsh_index (wall time)\n",
        "* recall@10 for faiss_lsh_ann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b4fdbd7671405821",
      "metadata": {
        "id": "b4fdbd7671405821"
      },
      "outputs": [],
      "source": [
        "query_vectors = np.load('data/query_vectors.npy')\n",
        "index_vectors = np.load('data/index_vectors.npy')\n",
        "k=10\n",
        "dim = index_vectors.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "qRfqqGOEG_E-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRfqqGOEG_E-",
        "outputId": "4f3b14e0-075e-4bd5-9300-5ee101d65988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "print(dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "65ff74d429524ffc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65ff74d429524ffc",
        "outputId": "47f56115-ac72-40e7-d378-e32b535af11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12 s, sys: 70.9 ms, total: 12.1 s\n",
            "Wall time: 17.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "gt_nn = semi_optimized_exhaustive_search(index_vectors, query_vectors, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bd448cbdb96b1ba0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd448cbdb96b1ba0",
        "outputId": "c8f488fc-14b5-493f-94e1-c8f41a182775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.55 s, sys: 157 ms, total: 1.71 s\n",
            "Wall time: 1.83 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "faiss_lsh_index = build_faiss_lsh_index(index_vectors, dim, nbits=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b0a321e6b7406267",
      "metadata": {
        "id": "b0a321e6b7406267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e0424f-60df-4dda-f51d-0acc1f947e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.28 s, sys: 10.1 ms, total: 1.29 s\n",
            "Wall time: 1.28 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "faiss_lsh_ann = faiss_search(query_vectors, faiss_lsh_index, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e5554595c4d77a27",
      "metadata": {
        "id": "e5554595c4d77a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f2bb5c-0797-4f4a-9048-ed94f0a142ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall@10 for faiss_lsh_index: 0.138\n"
          ]
        }
      ],
      "source": [
        "print(f\"recall@10 for faiss_lsh_index: {compute_recall_at_k(gt_nn, faiss_lsh_ann, k)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ca983b3a893e5",
      "metadata": {
        "id": "ad5ca983b3a893e5"
      },
      "source": [
        "# 2.2 -- Custom Indexing Algorithm\n",
        "Build an indexing algorithm that satisfies the following requirements:\n",
        "* The indexing algorithm should be able to handle vectors of different dimensions\n",
        "* The running time of the indexing should be less than half of the running time of semi_optimized_exhaustive_search), reported in Section 2.1.\n",
        "* The running time of searching over the index should be less than a third (1/3) of the time of the semi_optimized_exhaustive_search function, reported in Section 2.1.\n",
        "* The performance (in terms of recall@10) of the indexing algorithm should be at least 0.8.\n",
        "\n",
        "The last three bullets should also appear in the report.\n",
        "You are allowed to add as many helper functions as you need. You cannot use faiss of scipy libraries for this task. Numpy is allowed.\n",
        "\n",
        "You can also test your algorithm with the additional two query-index sets by replacing the calls made few cells ago to:\n",
        "\n",
        "    query_vectors = np.load('data/query_vectors2.npy')\n",
        "    index_vectors = np.load('data/index_vectors2.npy')\n",
        "or:\n",
        "\n",
        "    query_vectors = np.load('data/query_vectors3.npy')\n",
        "    index_vectors = np.load('data/index_vectors3.npy')\n",
        "    \n",
        "the aforementioned requirements should also be satisfied over these two query-index sets. No need to insert the results over these two to the report."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sixth edition\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn.cluster import MiniBatchKMeans as MBKM\n",
        "\n",
        "def brute_search(query_vector, index_vectors, k):\n",
        "  \"\"\"\n",
        "  This function apply brutal search of the top k vectors fitting to the query.\n",
        "  Args:\n",
        "      query_vector: A vector containing the query vectors.\n",
        "      index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "      k: The amount of vectors to retrieve.\n",
        "  Returns:\n",
        "      A closest - array of top k closest vectors to the query.\n",
        "  \"\"\"\n",
        "  # calculate the distances of the vectors to the query\n",
        "  distances = np.linalg.norm(index_vectors - query_vector, axis=1)\n",
        "\n",
        "  # take top closest vectors based on distances\n",
        "  closest = list(np.argsort(distances)[:k])\n",
        "\n",
        "  return closest\n",
        "\n",
        "def custom_indexing_algorithm(index_vectors, dim):\n",
        "  \"\"\"\n",
        "  This function builds an index from scratch.\n",
        "  Args:\n",
        "      index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "      dim: The dimensionality of the vectors.\n",
        "  Returns:\n",
        "      An index - including the kmeans clustering, the original data and the pca we converted with.\n",
        "  \"\"\"\n",
        "  # transform the data to pca space with dim=2\n",
        "  pca = decomposition.PCA()\n",
        "  pca.n_components = 2\n",
        "  pca_data = pca.fit_transform(index_vectors)\n",
        "\n",
        "  # cluster the data using MiniBatch Kmeans\n",
        "  clustering = MBKM(n_clusters=10)\n",
        "  clustering.fit(pca_data)\n",
        "\n",
        "  return clustering, index_vectors, pca\n",
        "\n",
        "def custom_index_search(query_vectors, index, k):\n",
        "  \"\"\"\n",
        "  This function searches over the custom index.\n",
        "  Args:\n",
        "      query_vectors: An array of shape (n_queries, dim) containing the query vectors.\n",
        "      index: The custom index.\n",
        "      k: The number of nearest neighbors to retrieve.\n",
        "  Returns:\n",
        "    An ann_lists - including list with top k vectors fitting to the query, for each query\n",
        "  \"\"\"\n",
        "  # list of lists for the results of the queries\n",
        "  ann_lists = []\n",
        "\n",
        "  # extract the indexing from the index we built\n",
        "  clustering, index_vectors, pca = index\n",
        "\n",
        "  # find the cluster of the queries\n",
        "  query_clusters = clustering.predict(pca.transform(query_vectors))\n",
        "\n",
        "  # for every query apply brutal search on the vectors from her class\n",
        "  for i, query in enumerate(query_vectors):\n",
        "    relevant_index = np.array(np.where(np.array(clustering.labels_)==query_clusters[i])[0]) #get the indexes of the vectors from the query's cluster\n",
        "    ann_lists.append(relevant_index[brute_search(query, index_vectors[relevant_index],k)])  #apply brutal search on all those vectors\n",
        "\n",
        "  return ann_lists"
      ],
      "metadata": {
        "id": "gS_pBTkQx2ir"
      },
      "id": "gS_pBTkQx2ir",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ef371ecd242846db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef371ecd242846db",
        "outputId": "27f285ac-5c23-4448-aa3d-aa87a2d06f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 349 ms, sys: 142 ms, total: 490 ms\n",
            "Wall time: 255 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "custom_index = custom_indexing_algorithm(index_vectors, dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1c40c61275a3d001",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c40c61275a3d001",
        "outputId": "992b05d9-9917-40a7-8190-b11f738b0e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.21 s, sys: 2.78 ms, total: 1.21 s\n",
            "Wall time: 1.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "custom_index_ann = custom_index_search(query_vectors, custom_index, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3ddba190c55cd0af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ddba190c55cd0af",
        "outputId": "825b29bc-9855-42a0-9c15-4d66f5919236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall@10 for custom_index_search: 1.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"recall@10 for custom_index_search: {compute_recall_at_k(gt_nn, custom_index_ann, k)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Previous Attemps\n",
        "\n",
        "code of privious attemps. no longer relevant since the latest attemp outperformed them."
      ],
      "metadata": {
        "id": "cep8nfMWnS1R"
      },
      "id": "cep8nfMWnS1R"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8421dc36363650c5",
      "metadata": {
        "id": "8421dc36363650c5"
      },
      "outputs": [],
      "source": [
        "# ### first edition\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def custom_indexing_algorithm(index_vectors, dim):\n",
        "#   \"\"\"\n",
        "#   This function builds an index from scratch.\n",
        "#   Args:\n",
        "#       index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "#       dim: The dimensionality of the vectors.\n",
        "#   Returns:\n",
        "#       An index.\n",
        "#   \"\"\"\n",
        "\n",
        "#   hp_num = 20   # constants\n",
        "\n",
        "#   # contain all the hash tables\n",
        "#   hash_tables = []\n",
        "\n",
        "#   # # randomly generate hp_num Hyper Planes\n",
        "#   # hp = np.random.rand(hp_num, dim)\n",
        "\n",
        "#   # Use PCA to generate hyperplanes\n",
        "#   pca = PCA(n_components=hp_num)\n",
        "#   pca.fit(index_vectors)\n",
        "#   hp = pca.components_\n",
        "\n",
        "#   # apply dot product to discover location of indexes\n",
        "#   dot_products = np.dot(index_vectors, hp.T)\n",
        "\n",
        "#   # Determine the hash value (1 if dot product > 0, else 0)\n",
        "#   hash_values = (dot_products > 0).astype(int)\n",
        "\n",
        "#   hash_table = defaultdict(list)  #start a hash table, Use defaultdict for efficient insertion\n",
        "\n",
        "#   # Go over the points and assign keys in the hash table\n",
        "#   for i, hash_val in enumerate(hash_values):\n",
        "#     hash_key = tuple(hash_val)  # Convert binary hash value to a tuple for use as a dictionary key\n",
        "#     hash_table[hash_key].append(i)  # Add the index of the point to the hash table at the right key\n",
        "\n",
        "#     # hash_tables.append((hash_table, hp))\n",
        "#   return hash_table, hp\n",
        "\n",
        "\n",
        "# def custom_index_search(query_vectors, index, k):\n",
        "#   \"\"\"\n",
        "#   This function searches over the custom index.\n",
        "#   Args:\n",
        "#       query_vectors: An array of shape (n_queries, dim) containing the query vectors.\n",
        "#       index: The custom index.\n",
        "#       k: The number of nearest neighbors to retrieve.\n",
        "#   \"\"\"\n",
        "#   # list of lists for the results of the queries\n",
        "#   ann_lists = []\n",
        "\n",
        "#   # extract the indexing from the index we built\n",
        "#   hash_table, hp = index\n",
        "\n",
        "#   # apply dot product to discover location of queries\n",
        "#   dot_products = np.dot(query_vectors, hp.T)\n",
        "\n",
        "#   # Determine the hash value (1 if dot product > 0, else 0)\n",
        "#   hash_values = (dot_products > 0).astype(int)\n",
        "\n",
        "#   for i, hash_val in enumerate(hash_values):\n",
        "#     hash_key = tuple(hash_val)  # Convert binary hash value to a tuple for use as a dictionary key\n",
        "#     if hash_key in hash_table:\n",
        "#       ann_lists.append(hash_table[hash_key][:k])\n",
        "#   return ann_lists"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### forth edition\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def brute_search(query_vector, index_vectors, k):\n",
        "#   print(len(index_vectors))\n",
        "#   distances = np.linalg.norm(index_vectors - query_vector, axis=1)\n",
        "#   closest = list(np.argsort(distances)[:k])\n",
        "#   return closest\n",
        "\n",
        "# def custom_indexing_algorithm(index_vectors, dim):\n",
        "#   \"\"\"\n",
        "#   This function builds an index from scratch.\n",
        "#   Args:\n",
        "#       index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "#       dim: The dimensionality of the vectors.\n",
        "#   Returns:\n",
        "#       An index.\n",
        "#   \"\"\"\n",
        "\n",
        "#   hp_num = 7  # constants\n",
        "\n",
        "#   # contain all the hash tables\n",
        "#   hash_tables = []\n",
        "\n",
        "#   # normaly generate hp_num Hyper Planes\n",
        "#   hp = np.random.multivariate_normal(np.zeros(dim), np.eye(dim), size=hp_num)\n",
        "\n",
        "#   # apply dot product to discover location of indexes\n",
        "#   dot_products = np.dot(index_vectors, hp.T)\n",
        "\n",
        "#   # Determine the hash value (1 if dot product > 0, else 0)\n",
        "#   hash_values = (dot_products > 0).astype(int)\n",
        "\n",
        "#   hash_table = defaultdict(list)  #start a hash table, Use defaultdict for efficient insertion\n",
        "\n",
        "#   # Go over the points and assign keys in the hash table\n",
        "#   for i, hash_val in enumerate(hash_values):\n",
        "#     hash_key = tuple(hash_val)  # Convert binary hash value to a tuple for use as a dictionary key\n",
        "#     hash_table[hash_key].append(i)  # Add the index of the point to the hash table at the right key\n",
        "\n",
        "#     # hash_tables.append((hash_table, hp))\n",
        "#   return hash_table, hp, index_vectors\n",
        "\n",
        "\n",
        "# def custom_index_search(query_vectors, index, k):\n",
        "#   \"\"\"\n",
        "#   This function searches over the custom index.\n",
        "#   Args:\n",
        "#       query_vectors: An array of shape (n_queries, dim) containing the query vectors.\n",
        "#       index: The custom index.\n",
        "#       k: The number of nearest neighbors to retrieve.\n",
        "#   \"\"\"\n",
        "#   # list of lists for the results of the queries\n",
        "#   ann_lists = []\n",
        "\n",
        "#   # extract the indexing from the index we built\n",
        "#   hash_table, hp, index_vectors = index\n",
        "\n",
        "#   # apply dot product to discover location of queries\n",
        "#   dot_products = np.dot(query_vectors, hp.T)\n",
        "\n",
        "#   # Determine the hash value (1 if dot product > 0, else 0)\n",
        "#   hash_values = (dot_products > 0).astype(int)\n",
        "\n",
        "#   for i, hash_val in enumerate(hash_values):\n",
        "#     hash_key = tuple(hash_val)  # Convert binary hash value to a tuple for use as a dictionary key\n",
        "#     if hash_key in hash_table:\n",
        "#       relevant_index = np.array(hash_table[hash_key])\n",
        "#       ann_lists.append(relevant_index[brute_search(query_vectors[i], index_vectors[relevant_index],k)])\n",
        "#   return ann_lists"
      ],
      "metadata": {
        "id": "65T2NDQAW01I"
      },
      "id": "65T2NDQAW01I",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###fifth edition\n",
        "\n",
        "# def brute_search(query_vector, index_vectors, k):\n",
        "#   distances = np.linalg.norm(index_vectors - query_vector, axis=1)\n",
        "#   closest = list(np.argsort(distances)[:k])\n",
        "#   return closest\n",
        "\n",
        "# def custom_indexing_algorithm(index_vectors, dim):\n",
        "#   \"\"\"\n",
        "#   This function builds an index from scratch.\n",
        "#   Args:\n",
        "#       index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
        "#       dim: The dimensionality of the vectors.\n",
        "#   Returns:\n",
        "#       An index.\n",
        "#   \"\"\"\n",
        "#   (idx_num, hp_num) = 50, 7     # constants\n",
        "\n",
        "#   (best_hash_table, best_measure) = defaultdict(list), np.inf    # contain the best hash table\n",
        "\n",
        "#   #loop for several indexes  ------temporarily start with one hase table.\n",
        "#   for _ in range(idx_num):\n",
        "\n",
        "#     # normaly generate hp_num Hyper Planes\n",
        "#     hp = np.random.multivariate_normal(np.zeros(dim), np.eye(dim), size=hp_num)\n",
        "\n",
        "#     # apply dot product to discover location of indexes\n",
        "#     dot_products = np.dot(index_vectors, hp.T)\n",
        "\n",
        "#     # Determine the hash value (1 if dot product > 0, else 0)\n",
        "#     hash_values = (dot_products > 0).astype(int)\n",
        "\n",
        "#     hash_table = defaultdict(list)  #start a hash table, Use defaultdict for efficient insertion\n",
        "\n",
        "#     # Go over the points and assign keys in the hash table\n",
        "#     for i, hash_val in enumerate(hash_values):\n",
        "#       hash_key = tuple(hash_val)  # Convert binary hash value to a tuple for use as a dictionary key\n",
        "#       hash_table[hash_key].append(i)  # Add the index of the point to the hash table at the right key\n",
        "\n",
        "#     longest = max(len(item) for item in hash_table.values())\n",
        "\n",
        "#     if best_measure>(longest):\n",
        "#       best_hash_table, best_measure = (hash_table, hp), longest\n",
        "\n",
        "#   return best_hash_table, index_vectors\n",
        "\n",
        "\n",
        "# def custom_index_search(query_vectors, index, k):\n",
        "#   \"\"\"\n",
        "#   This function searches over the custom index.\n",
        "#   Args:\n",
        "#       query_vectors: An array of shape (n_queries, dim) containing the query vectors.\n",
        "#       index: The custom index.\n",
        "#       k: The number of nearest neighbors to retrieve.\n",
        "#   \"\"\"\n",
        "#   # list of lists for the results of the queries\n",
        "#   ann_lists = []\n",
        "\n",
        "#   # extract the indexing from the index we built\n",
        "#   (hash_table, hp), index_vectors = index\n",
        "\n",
        "#   # apply dot product to discover location of queries\n",
        "#   dot_products = np.dot(query_vectors, hp.T)\n",
        "\n",
        "#   # Determine the hash value (1 if dot product > 0, else 0)\n",
        "#   hash_values = (dot_products > 0).astype(int)\n",
        "\n",
        "#   for i, hash_val in enumerate(hash_values):\n",
        "#     hash_key = tuple(hash_val)  # Convert binary hash value to a tuple for use as a dictionary key\n",
        "#     if hash_key in hash_table:\n",
        "#       relevant_index = np.array(hash_table[hash_key])\n",
        "#       ann_lists.append(relevant_index[brute_search(query_vectors[i], index_vectors[relevant_index],k)])\n",
        "#   return ann_lists"
      ],
      "metadata": {
        "id": "n-Jy31X1i-mb"
      },
      "execution_count": 21,
      "outputs": [],
      "id": "n-Jy31X1i-mb"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PUMcJr3G92vb",
        "62f09c59a02a3b0d",
        "77d4be2e90ed842"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}